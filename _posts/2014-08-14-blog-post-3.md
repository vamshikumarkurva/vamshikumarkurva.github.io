---
title: 'Variational Auto Encoder'
date: 2020-05-02
mathjax: true
permalink: /posts/2020/05/VAE/
tags:
  - latent variable model
  - Bayesian method
---

This blogpost talks about modelling the distribution of images, the challenges in modelling and training etc.  Let's try to fit a model $$p(x)$$ to a set of images, just like we tried to fit GMM to a set of points in some dimensional space.

why model $$p(x)$$ for images?
======
Modelling the distribution of images has the following advantages.
 1. We can represent the data in a compact form using model parameters. Typically the latent dimension is much lesser compared to the input dimension.
 2. We can use the model to generate new samples (images in this case).
 3. Model can be used to detect anomalies/outliers by predicting the probability of the data point according to the model.

How to model distribution of images?
------

1. Use the CNN to model the images,i.e.

\\[
\begin{aligned}
    \log \hat{p}(x) &= CNN(x) \newline
    p(x) &= \frac{\exp(CNN(x))}{Z} \newline
\end{aligned}
\\]

- The problem is the normalization constant, the sum of probabilities should be 1 w.r.to all possible images. i.e. to calculate $$Z$$ we have to use every possible natural image. So, this approach is not feasible

2. Use chain rule and model conditional distribution

$$
    p(x_1, x_2,...x_n) = p(x_1) p(x_2/x_1)...p(x_n/x_1,x_2...x_{n-1})
$$

- The joint distribution over the pixels can be modelled using the conditional distributions. For a $$100 X 100$$ gray scale image, the model is a factorized over 10k conditional distributions. So, each conditional distribution needs to be normalized using only 256 values. We can model any probability distribution in this way. Conditional distribution can be modelled using RNN as

$$
    p(x_n/x_1,x_2...x_{n-1}) = RNN(x_1,x_2...x_{n-1})
$$

The model reads the image pixel by pixel and generates the distribution over the next pixel. This model takes too long to generate even a low resolution image.

3. We can model the distribution of pixels in an image as independent, but since that's not the case, the images generated by the model doesn't resemble the images in the data set.

4. Mixture of several Gaussians (GMM)

- Theoretically GMM can model any distribution, but in practice it can be inefficient for complicated data sets like images. GMM in this case may fail to capture the structure in the data, since it's too difficult to train.

5. Mixture of infinitely many Gaussians

$$
    p(x) = \int p(x/t) p(t) dt
$$

- Here $$t$$ is a latent variable, and $t$ is assumed to cause $$x$$. $$p(x/t)$$ can be modelled as Gaussian. Even if the Gaussians are factorized, i.e. have independent components for each dimension, mixture is not. Hence, this model is a bit more powerful than the GMM model.

