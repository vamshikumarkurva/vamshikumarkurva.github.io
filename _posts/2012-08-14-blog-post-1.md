---
title: 'Latent Variables and EM'
date: 2020-04-30
mathjax: true
permalink: /posts/2020/04/Latent_Variables_EM/
tags:
  - latent variable models
---

This blog post talks about latent variables, why we need them and how to train latent variable models.

Latent Variables
======

Latent variable is a variable which is never observed (hidden). Why do we need them?
1. Our data might contain missing values
2. We want to know about the uncertainty in our predictions.

Assume you are an HR in a company, and you want to call eligible candidates for interview. Also assume that thevariables for consideration are somethings like GPA, IQ score, Aptitude test score, high school grade etc. Not allcandidates might have taken all the tests, and hence data might contain missing values. Also, we want to quantifyuncertainty in our predictions, so that we can make better decisions about whom to invite for interview. For example,if a candidate is predicted to be a not good fit for the interview, but with high uncertainty, we may as well invitehim for the interview since we are not sure he is not a good fit. To handle these kinds of data we need probabilisticmodels.

Probabilistic model
======

Suppose $$X_1, X_2, X_3$$ are random variables that depends on one another. Now let’s assume each of them taken on one of 100, 200, 300 values respectively. So, the total no of combinations of all variables are 6 million. We can model the joint probability distribution as

$$
        p(x_1, x_2, x_3) = \frac{\exp(-w^Tx)}{Z}
$$

Where $$Z$$ is the normalization constant, which is a sum over all 6 million combinations. This makes the training and inference impractical.

Now let’s introduce a new variable $t$, which we call latent variable and $t$ causes each of RVs $$X_1,X_2,X_3$$. Then,

\\[
\begin{aligned}
    p(x_1,x_2,x_3) &= \int p(x_1, x_2, x_3/t) p(t) dt \newline
                    &= \int p(x_1/t) p(x_2/t) p(x_3/t)p(t) dt \hspace{2ex}\text{(because of independence)}
\end{aligned}
\\]

Each conditional distribution is easy to model now, as the distribution is over 100, 200, 300 values respectively. So, the model complexity is reduced without compromising the flexibility.
Probabilistic models can be used as generating models and can also be used to project data onto a low dimensional latent space.

Probabilistic clustering
------

This is a soft clustering mechanism, which assigns each data point the probability of belonging to a particular cluster. This helps in classifying uncertainty in our cluster assignment.


Gaussian Mixture Model (GMMs)
------

When the data contains several sets of clusters, fitting a Gaussian model to the data is not the best way. Rather, we can use a mixture of Gaussian's which can well model most types of clusters.

<p align="center">
  <img src="https://github.com/vamshikumarkurva/vamshikumarkurva.github.io/blob/master/_posts/Images/single_gaussian.png?raw=true" alt="Photo" style="width: 450px;"/>
</p>
![](Images/single_gaussian.png)

As you can see in the above image, there are clearly 3 clusters, and a single gaussian fit to the data won't be able to capture the structure in the data. We can use a mixture of 3 gaussians
<p align="center">
  <img src="https://github.com/vamshikumarkurva/vamshikumarkurva.github.io/blob/master/_posts/Images/multiple_gaussian.png?raw=true" alt="Photo" style="width: 450px;"/>
</p>
![](Images/multiple_gaussian.png)

$$
    p(x/\theta) = \sum_{i=1}^{3} \pi_i \mathcal{N}(\mu_i, \sigma_i)
$$

where $$\theta = \{\pi_1, \pi_2, \pi_3, \mu_1, \mu_2, \mu_3, \sigma_1, \sigma_2, \sigma_3\}$$, are parameters of the model.

If we find the parameters $$\theta$$, we can know which cluster each data point came from, also we can generate new data using these parameters. All we need to do is sample from the distribution.
How to find parameters? Use maximum likelihood

Training GMMs
------
Using maximum likelihood principle, the optimization problem is

\\[
\begin{aligned}
\max_{\theta} \quad & p(X/\theta) = \prod_{i=1}^N p(x_i/\theta) = \prod_{i=1}^{N} \sum_{k=1}^{C} \pi_k \mathcal{N}(\mu_k, \sigma_k)\newline
\textrm{s.t.} \quad & \sum_{k=1}^C \pi_k = 1; \pi_k \geq 0\newline
  &\sigma_i > 0    \newline
\end{aligned} 
\\]

We can use SGD to maximize the objective, but it is difficult to enforce the positive definiteness constraint.
Let’s introduce a latent variable $$t$$ here and assume that each data point $$x$$ is generated using some information from $$t$$ i.e. $$t$$ causes $$x$$. Here $$t \in \{1,2,..C\}$$, that is it tells us which Gaussian the given data point come from. $$t$$ is a latent variable, we don’t observe it.

Let the prior distribution on $t$ is $$\pi$$. i.e. $$p(t=c/\theta)= \pi_c$$

The likelihood of  the point $$x$$ belonging to the cluster $$c$$ is $$p(x/t=c,\theta)=  \mathcal{N}(\mu_c,\sigma_c) $$ 

Now  the likelihood under the mixture of Gaussins is

$$
p(x/\theta) = \sum_{k=1}^{C}p(x/t=c,\theta)  p(t=c/\theta)= \sum_{k=1}^{C} \pi_k \mathcal{N}(\mu_k, \sigma_k)
$$

i.e. introducing latent variable has not changed the model. How to estimate parameters $$\theta$$? 

If we know the sources $$t$$, i.e. $$p(t_i/x_i, \theta)$$, we can find the parameters $$\theta$$. i.e. in one dimensional case,

$$
    \mu_c = \frac{\sum_{i} p(t_i=c/x_i, \theta) x_i}{\sum_{i} p(t_i=c/x_i, \theta)}
$$

$$
    \sigma_c = \frac{\sum_{i} p(t_i=c/x_i, \theta) (x_i-\mu_c)^2}{\sum_{i} p(t_i=c/x_i, \theta)}
$$

If we know the parameters $$\theta$$, i.e we can find the sources $$t$$

$$
    p(t=c/x, \theta) = \frac{p(x/t=c, \theta)p(t=c/\theta)}{\sum_{c=1}^C p(x/t=c, \theta)p(t=c/\theta)} 
$$

This is basically a chicken and egg problem. We know neither parameters nor sources.

Expectation-Maximization algorithm
------

1.Start with random Gaussian params $$\theta$$

2.Until convergence, repeat

&nbsp;&nbsp;a). For each point compute $$p(t_i = c/x_i, \theta)$$, probability that the given point comes from cluster c. 

&nbsp;&nbsp;b). Update Gaussian parameters $$\theta$$, to fit points assigned to them.
  
EM algorithm can be faster than SGD and handles the complicated constraints.
EM algorithm suffers from local maxima, i.e. different initialization can lead to different solutions.

Concave-function
------

A function $$f(x)$$ is concave, if $$\forall a, b, 0 \leq \alpha \leq 1$$
$$
    f(\alpha a + (1-\alpha b))  \geq \alpha f(a) + (1-\alpha) f(b)
$$

<p align="center">
  <img src="https://github.com/vamshikumarkurva/vamshikumarkurva.github.io/blob/master/_posts/Images/concave.png?raw=true" alt="Photo" style="width: 450px;"/>
</p>
![](Images/concave.png)

In general $$\forall a_1, a_2,..a_n$$

$$
    f(\alpha_1 a_1+\alpha_2 a_2+...+\alpha_n a_n) \geq \alpha_1 f(a_1) + \alpha_2 f(a_2)+...+ \alpha_n f(a_n) \label{eq1}
$$

If we assume a RV $$t$$, such that $$p(t=a_i)= \alpha_i$$, then the inequality (\ref{eq1}) can be written as

$$
    f(\mathbf{E}_{p(t)} t ) \geq \mathbf{E}_{p(t)} (f(t)) \label{eq2}
$$
	
(\ref{eq2}) is called Jensen's inequality.

$f(x) = \log(x)$ is a concave function. Concave functions have only one global maxima.

Kullback-Leibler Divergence
------
 <p align="center">
  <img src="https://github.com/vamshikumarkurva/vamshikumarkurva.github.io/blob/master/_posts/Images/KL_example.png?raw=true" alt="Photo" style="width: 450px;"/>
</p>
![](Images/KL_example.png)

In the above image, even though the parametric difference between the distributions is same, the second set of distributions are closer to each other when compared to that of first set. KL divergence would reflect this in its value.

KL divergence is a measure of how a probability distribution is different from a reference probability distribution. For $$p$$ and $$q$$, discrete probability distributions on the same space $$\mathcal{X}$$, it is defined as
 
 \\[
 \begin{aligned}
     KL(q || p) &= \sum_{x \in \mathcal{X}} q(x) log (\frac{q(x)}{p(x)}) \newline
                &= \mathbf{E}_{q} log(\frac{q}{p})
 \end{aligned}
 \\]

In other words, it is the expectation of the logarithmic difference between the probabilities $$q$$ and $$p$$, where the expectation is taken using the probabilities $$q$$. For continuous distributions, sum is replaced by integration. Some properties are

1. KL divergence measure is not symmetric, and hence not a metric. 
$$KL(q || p) \neq KL(p || q)$$.
2. For identical distributions $$q$$ and $$p$$,
$$KL(q || p) = 0$$
3. Non-negative, 
$$KL(q || p) \geq 0$$






 





