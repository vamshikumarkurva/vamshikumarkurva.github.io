---
title: 'Latent Variables and EM'
date: 2020-04-30
mathjax: true
permalink: /posts/2020/04/Latent_Variables_EM/
tags:
  - latent variable models
---

This blog post talks about latent variables, why we need them and how to train latent variable models.

Latent Variables
======

Latent variable is a variable which is never observed (hidden). Why do we need them?
1. Our data might contain missing values
2. We want to know about the uncertainty in our predictions.

Assume you are an HR in a company, and you want to call eligible candidates for interview. Also assume that thevariables for consideration are somethings like GPA, IQ score, Aptitude test score, high school grade etc. Not allcandidates might have taken all the tests, and hence data might contain missing values. Also, we want to quantifyuncertainty in our predictions, so that we can make better decisions about whom to invite for interview. For example,if a candidate is predicted to be a not good fit for the interview, but with high uncertainty, we may as well invitehim for the interview since we are not sure he is not a good fit. To handle these kinds of data we need probabilisticmodels.

Probabilistic model
======

Suppose $$X_1, X_2, X_3$$ are random variables that depends on one another. Now let’s assume each of them taken on one of 100, 200, 300 values respectively. So, the total no of combinations of all variables are 6 million. We can model the joint probability distribution as

$$
        p(x_1, x_2, x_3) = \frac{\exp(-w^Tx)}{Z}
$$

Where $$Z$$ is the normalization constant, which is a sum over all 6 million combinations. This makes the training and inference impractical.

Now let’s introduce a new variable $t$, which we call latent variable and $t$ causes each of RVs $$X_1,X_2,X_3$$. Then,

\\[
\begin{aligned}
    p(x_1,x_2,x_3) &= \int p(x_1, x_2, x_3/t) p(t) dt \newline
                    &= \int p(x_1/t) p(x_2/t) p(x_3/t)p(t) dt \hspace{2ex}\text{(because of independence)}
\end{aligned}
\\]

Each conditional distribution is easy to model now, as the distribution is over 100, 200, 300 values respectively. So, the model complexity is reduced without compromising the flexibility.
Probabilistic models can be used as generating models and can also be used to project data onto a low dimensional latent space.

Probabilistic clustering
------

This is a soft clustering mechanism, which assigns each data point the probability of belonging to a particular cluster. This helps in classifying uncertainty in our cluster assignment.


Gaussian Mixture Model (GMMs)
------

When the data contains several sets of clusters, fitting a Gaussian model to the data is not the best way. Rather, we can use a mixture of Gaussian's which can well model most types of clusters.

<img src="Images/single_gaussian.png" width="500">

As you can see in the above image, there are clearly 3 clusters, and a single gaussian fit to the data won't be able to capture the structure in the data. We can use a mixture of 3 gaussians

$$
    p(x/\theta) = \sum_{i=1}^{3} \pi_i \mathcal{N}(\mu_i, \sigma_i)
$$

where $$\theta = \{\pi_1, \pi_2, \pi_3, \mu_1, \mu_2, \mu_3, \sigma_1, \sigma_2, \sigma_3\}$$, are parameters of the model

